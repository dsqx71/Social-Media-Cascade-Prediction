{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'preprocessing' from 'preprocessing.pyc'>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "#import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import os.path\n",
    "import access_data \n",
    "import preprocessing\n",
    "import setting\n",
    "import feature\n",
    "import sys\n",
    "import time\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.ensemble import AdaBoostClassifier,AdaBoostRegressor,\\\n",
    "RandomForestClassifier,RandomForestRegressor,GradientBoostingClassifier,GradientBoostingRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor,DecisionTreeClassifier\n",
    "\n",
    "pd.set_option('display.max_columns' ,1000)\n",
    "pd.set_option('display.max_rows',60)\n",
    "%matplotlib inline\n",
    "reload(feature)\n",
    "reload(access_data)\n",
    "reload(preprocessing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#清理 + 分词 + label encode+计算词频"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2856368, 7) (178297, 4)\n"
     ]
    }
   ],
   "source": [
    "print train.shape,test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:finished cleaning symbol http[0-9a-zA-Z?:=._@%/\\-#&\\+|]+\n",
      "INFO:root:finished cleaning symbol //@\n",
      "INFO:root:finished cleaning symbol @\n",
      "INFO:root:finished cleaning symbol #\n",
      "INFO:root:finished cleaning symbol 【\n",
      "INFO:root:finished cleaning symbol 《\n",
      "INFO:root:finished cleaning symbol \\[\n",
      "INFO:root:Start to segment\n",
      "Building prefix dict from C:\\Anaconda\\lib\\site-packages\\jieba\\dict.txt ...\n",
      "DEBUG:jieba:Building prefix dict from C:\\Anaconda\\lib\\site-packages\\jieba\\dict.txt ...\n",
      "Loading model from cache c:\\users\\admini~1\\appdata\\local\\temp\\jieba.cache\n",
      "DEBUG:jieba:Loading model from cache c:\\users\\admini~1\\appdata\\local\\temp\\jieba.cache\n",
      "Loading model cost 0.955 seconds.\n",
      "DEBUG:jieba:Loading model cost 0.955 seconds.\n",
      "Prefix dict has been built succesfully.\n",
      "DEBUG:jieba:Prefix dict has been built succesfully.\n",
      "INFO:root:have segmented 0\n",
      "INFO:root:have segmented 100000\n",
      "INFO:root:Start to segment\n",
      "INFO:root:have segmented 0\n",
      "INFO:root:have segmented 100000\n",
      "INFO:root:have segmented 200000\n",
      "INFO:root:have segmented 300000\n",
      "INFO:root:have segmented 400000\n",
      "INFO:root:have segmented 500000\n",
      "INFO:root:have segmented 600000\n",
      "INFO:root:have segmented 700000\n",
      "INFO:root:have segmented 800000\n",
      "INFO:root:have segmented 900000\n",
      "INFO:root:have segmented 1000000\n",
      "INFO:root:have segmented 1100000\n",
      "INFO:root:have segmented 1200000\n",
      "INFO:root:have segmented 1300000\n",
      "INFO:root:have segmented 1400000\n",
      "INFO:root:have segmented 1500000\n",
      "INFO:root:have segmented 1600000\n",
      "INFO:root:have segmented 1700000\n",
      "INFO:root:have segmented 1800000\n",
      "INFO:root:have segmented 1900000\n",
      "INFO:root:have segmented 2000000\n",
      "INFO:root:have segmented 2100000\n",
      "INFO:root:have segmented 2200000\n",
      "INFO:root:have segmented 2300000\n",
      "INFO:root:have segmented 2400000\n",
      "INFO:root:have segmented 2500000\n",
      "INFO:root:have segmented 2600000\n",
      "INFO:root:have segmented 2700000\n",
      "INFO:root:have segmented 2800000\n"
     ]
    }
   ],
   "source": [
    "train_corpus ,test_corpus = preprocessing.clean_corpus()\n",
    "test_corpus['corpus'] = preprocessing.segment_word(test_corpus['corpus'])\n",
    "train_corpus['corpus'] = preprocessing.segment_word(train_corpus['corpus'])\n",
    "\n",
    "train_corpus.to_pickle(setting.processed_data_dir+'cleaned&segment_train')\n",
    "test_corpus.to_pickle(setting.processed_data_dir+'cleaned&segment_test')\n",
    "\n",
    "train_uid,test_uid = preprocessing.encode_label()\n",
    "\n",
    "preprocessing.bag_of_word(train_corpus['corpus'].values,test_corpus['corpus'].values,min_df=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##主题模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Start to run_lda.py\n",
      "INFO:lda:n_documents: 3034665\n",
      "INFO:lda:vocab_size: 138197\n",
      "INFO:lda:n_words: 40790889\n",
      "INFO:lda:n_topics: 25\n",
      "INFO:lda:n_iter: 500\n",
      "WARNING:lda:all zero row in document-term matrix found\n",
      "INFO:lda:<0> log likelihood: -571325418\n",
      "INFO:lda:<10> log likelihood: -426492084\n",
      "INFO:lda:<20> log likelihood: -382759562\n",
      "INFO:lda:<30> log likelihood: -373764722\n",
      "INFO:lda:<40> log likelihood: -370693337\n",
      "INFO:lda:<50> log likelihood: -369113446\n",
      "INFO:lda:<60> log likelihood: -368217278\n",
      "INFO:lda:<70> log likelihood: -367628667\n",
      "INFO:lda:<80> log likelihood: -367153086\n",
      "INFO:lda:<90> log likelihood: -366866050\n",
      "INFO:lda:<100> log likelihood: -366647520\n",
      "INFO:lda:<110> log likelihood: -366440536\n",
      "INFO:lda:<120> log likelihood: -366305537\n",
      "INFO:lda:<130> log likelihood: -366166859\n",
      "INFO:lda:<140> log likelihood: -366072189\n",
      "INFO:lda:<150> log likelihood: -366000983\n",
      "INFO:lda:<160> log likelihood: -365908501\n",
      "INFO:lda:<170> log likelihood: -365851226\n",
      "INFO:lda:<180> log likelihood: -365807118\n",
      "INFO:lda:<190> log likelihood: -365742755\n",
      "INFO:lda:<200> log likelihood: -365643783\n",
      "INFO:lda:<210> log likelihood: -365601654\n",
      "INFO:lda:<220> log likelihood: -365579825\n",
      "INFO:lda:<230> log likelihood: -365563843\n",
      "INFO:lda:<240> log likelihood: -365539831\n",
      "INFO:lda:<250> log likelihood: -365497508\n",
      "INFO:lda:<260> log likelihood: -365481551\n",
      "INFO:lda:<270> log likelihood: -365453372\n",
      "INFO:lda:<280> log likelihood: -365436545\n",
      "INFO:lda:<290> log likelihood: -365411166\n",
      "INFO:lda:<300> log likelihood: -365399251\n",
      "INFO:lda:<310> log likelihood: -365384506\n",
      "INFO:lda:<320> log likelihood: -365381183\n",
      "INFO:lda:<330> log likelihood: -365368830\n",
      "INFO:lda:<340> log likelihood: -365358975\n",
      "INFO:lda:<350> log likelihood: -365340225\n",
      "INFO:lda:<360> log likelihood: -365340224\n",
      "INFO:lda:<370> log likelihood: -365338691\n",
      "INFO:lda:<380> log likelihood: -365304064\n",
      "INFO:lda:<390> log likelihood: -365308683\n",
      "INFO:lda:<400> log likelihood: -365279917\n",
      "INFO:lda:<410> log likelihood: -365275485\n",
      "INFO:lda:<420> log likelihood: -365277722\n",
      "INFO:lda:<430> log likelihood: -365271460\n",
      "INFO:lda:<440> log likelihood: -365260359\n",
      "INFO:lda:<450> log likelihood: -365250797\n",
      "INFO:lda:<460> log likelihood: -365189670\n",
      "INFO:lda:<470> log likelihood: -365186392\n",
      "INFO:lda:<480> log likelihood: -365142376\n",
      "INFO:lda:<490> log likelihood: -365096986\n",
      "INFO:lda:<499> log likelihood: -365064645\n"
     ]
    },
    {
     "ename": "UnicodeEncodeError",
     "evalue": "'ascii' codec can't encode characters in position 0-1: ordinal not in range(128)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnicodeEncodeError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[1;32mF:\\socialnetwork\\competition\\run_lda.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtopic_dist\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtopic_word\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m     \u001b[0mtopic_words\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margsort\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtopic_dist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mn_top_words\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 27\u001b[1;33m     \u001b[1;32mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Topic {}: {}'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m' '\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtopic_words\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     28\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mUnicodeEncodeError\u001b[0m: 'ascii' codec can't encode characters in position 0-1: ordinal not in range(128)"
     ]
    }
   ],
   "source": [
    "%run run_lda.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  ##构建用户基本特征 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_uid = access_data.load_processed_data('uid&pid_train')\n",
    "test_uid  = access_data.load_processed_data('uid&pid_test')\n",
    "\n",
    "train ,test = access_data.load_raw_data()\n",
    "\n",
    "train_corpus = access_data.load_processed_data('cleaned&segment_train')\n",
    "test_corpus  = access_data.load_processed_data('cleaned&segment_test')                                               \n",
    "train = pd.concat([train_uid,train,train_corpus],axis=1)\n",
    "test  = pd.concat([test_uid,test,test_corpus],axis=1)\n",
    "\n",
    "train.drop([0,1],axis=1,inplace=True)\n",
    "test.drop([0,1],axis=1,inplace=True)\n",
    "\n",
    "train.columns = ['pid','uid','time','share','comment','zan','raw_corpus','clean&segment','链接','//@','@','#','【','《','\\[']\n",
    "test.columns = ['pid','uid','time','raw_corpus','clean&segment','链接','//@','@','#','【','《','\\[']\n",
    "train['uid'] = train['uid'].astype(np.uint16)\n",
    "test['uid']  = test['uid'].astype(np.uint16)\n",
    "l = ['链接','//@','@','#','【','《','\\[']\n",
    "\n",
    "for string in l :\n",
    "    train[string] = train[string].astype(np.int8)\n",
    "    test[string]  = test[string].astype(np.int8)\n",
    "\n",
    "#在training set和test set中和用户发送微博的总数量\n",
    "tot = pd.concat([pd.DataFrame(train['uid']),pd.DataFrame(test['uid'])])\n",
    "c = pd.DataFrame(tot['uid'].value_counts())\n",
    "c.columns = ['tot_counts']\n",
    "train = train.merge(c,left_on='uid',right_index=True,how='left')\n",
    "test  = test.merge(c,left_on='uid',right_index=True,how='left')\n",
    "\n",
    "# 用户出现在训练集的次数\n",
    "c = pd.DataFrame(train['uid'].value_counts())\n",
    "c.columns = ['train_counts']\n",
    "train = train.merge(c,left_on='uid',right_index=True,how='left')\n",
    "test  = test.merge(c,left_on='uid',right_index=True,how='left')\n",
    "\n",
    "test.fillna(-1,inplace=True)\n",
    "train['tot_counts'] = train['tot_counts'].astype(np.int32)\n",
    "train['train_counts'] = train['train_counts'].astype(np.int32)\n",
    "\n",
    "test['tot_counts'] = test['tot_counts'].astype(np.int32)\n",
    "test['train_counts'] = test['train_counts'].astype(np.int32)\n",
    "\n",
    "addr1 = setting.raw_data_dir + 'basic_train'\n",
    "addr2 = setting.raw_data_dir + 'basic_test'\n",
    "\n",
    "lda_result = np.load('processed_data/lda_result_version3.npy')\n",
    "lda_result = pd.DataFrame(lda_result,columns=['topic_%d' %i for i in range(0,25)])\n",
    "\n",
    "for string in ['topic_%d' %i for i in range(0,25)]:\n",
    "    train[string] = lda_result.loc[:train.shape[0]-1,string].values\n",
    "    test[string] = lda_result.loc[train.shape[0]:,string].values\n",
    "\n",
    "train.to_pickle(addr1)\n",
    "test.to_pickle(addr2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##用户特征 + 时间特征  + 文本特征"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_basic = pd.read_pickle('raw_data/basic_train')\n",
    "test_basic  = pd.read_pickle('raw_data/basic_test')\n",
    "#train_basic = train_basic.loc[1626750:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "reload(feature)\n",
    "#计算情感极性\n",
    "begin_time= time.time()\n",
    "train_sentiment,test_sentiment = feature.sentiment_feature(train_basic,test_basic)\n",
    "end_time = time.time()\n",
    "print end_time - begin_time\n",
    "#计算一周内出现微博的数量\n",
    "begin_time= time.time()\n",
    "train_seven_days ,test_seven_days = feature.find_seven_days(train_basic,test_basic)\n",
    "end_time = time.time()\n",
    "print end_time - begin_time\n",
    "#lda特征\n",
    "begin_time = time.time()\n",
    "train_lda_feature,test_lda_feature = feature.lda_feature(train_basic,test_basic)\n",
    "end_time = time.time()\n",
    "print end_time - begin_time\n",
    "#用户特征\n",
    "begin_time= time.time()\n",
    "train_user,test_user = feature.user_basic_feature(train_basic,test_basic)\n",
    "end_time = time.time()\n",
    "print end_time - begin_time\n",
    "#文本特征\n",
    "begin_time= time.time()\n",
    "train_content,test_content = feature.content_basic_feature(train_basic,test_basic)\n",
    "end_time = time.time()\n",
    "print end_time - begin_time\n",
    "#时间特征\n",
    "begin_time= time.time()\n",
    "train_time,test_time = feature.time_feature(train_basic,test_basic)\n",
    "end_time = time.time()\n",
    "print end_time - begin_time\n",
    "#关键词特征\n",
    "begin_time = time.time()\n",
    "train_keyword,test_keyword = feature.key_word_feature(train_basic,test_basic)\n",
    "end_time = time.time()\n",
    "print end_time - begin_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_basic.drop(['uid','pid','time','share','comment','zan','raw_corpus','clean&segment'],axis=1,inplace=True)\n",
    "test_basic.drop(['uid','pid','time','raw_corpus','clean&segment'],axis=1,inplace=True)\n",
    "train_user = pd.concat([train_user,train_basic],axis=1)\n",
    "test_user  = pd.concat([test_user,test_basic],axis=1)\n",
    "\n",
    "train = train_user.merge(train_content,how='left',left_on='pid',right_index=True)\n",
    "test  = test_user.merge(test_content,how='left',left_on='pid',right_index=True)\n",
    "\n",
    "train = train.merge(train_time,how='left',left_on='pid',right_index=True)\n",
    "test  = test.merge(test_time,how='left',left_on='pid',right_index=True)\n",
    "\n",
    "train = pd.concat([train,train_keyword],axis=1)\n",
    "test  = pd.concat([test,test_keyword],axis=1)\n",
    "\n",
    "train = train.merge(train_lda_feature,how='left',left_on='pid',right_index=True)\n",
    "test  = test.merge(test_lda_feature,how='left',left_on='pid',right_index=True)\n",
    "\n",
    "train = train.merge(train_sentiment,how='left',left_on='pid',right_index=True)\n",
    "test  = test.merge(test_sentiment,how='left',left_on='pid',right_index=True)\n",
    "\n",
    "train = train.merge(train_seven_days,how='left',left_on='pid',right_index=True)\n",
    "test  = test.merge(test_seven_days,how='left',left_on='pid',right_index=True)\n",
    "\n",
    "begin_time = time.time()\n",
    "result,gmm = feature.clustering_feature(train,test)\n",
    "end_time = time.time()\n",
    "print end_time - begin_time\n",
    "\n",
    "train = train.merge(result,how='left',left_on='uid',right_index=True)\n",
    "test  = test.merge(result,how='left',left_on='uid',right_index=True)\n",
    "\n",
    "train.to_pickle('processed_data/train3')\n",
    "test.to_pickle('processed_data/test3')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "##整理格式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train = pd.read_pickle('processed_data/train2')\n",
    "test  = pd.read_pickle('processed_data/test2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#train.drop(['sentiment','seven_days'],axis=1,inplace=True)\n",
    "#test.drop(['sentiment','seven_days'],axis=1,inplace=True)\n",
    "result_test = []\n",
    "result_train = []\n",
    "tot = 0\n",
    "for string in ['share','comment','zan','content_len','链接','//@','@','#','【','《','\\[']:\n",
    "    temp = []\n",
    "    for i in test[string+'_histogram']:\n",
    "        if isinstance(i,int):\n",
    "            temp.append(np.zeros(shape=8))\n",
    "            tot +=1\n",
    "        else:\n",
    "            temp.append(i[0])\n",
    "    result_test.append(np.asarray(temp))\n",
    "    temp = []\n",
    "    for i in train[string+'_histogram']:\n",
    "        temp.append(i[0])\n",
    "    result_train.append(np.asarray(temp))\n",
    "    \n",
    "    train.drop(string+'_histogram',axis=1,inplace=True)\n",
    "    test.drop(string+'_histogram',axis=1,inplace=True)\n",
    "train.drop(['pid','uid'],inplace=True,axis = 1)\n",
    "test.drop(['pid','uid'],inplace=True,axis = 1)\n",
    "\n",
    "train_y = train[['share','comment','zan']].values\n",
    "train.drop(['share','comment','zan'],axis = 1,inplace=True)\n",
    "train_x = train.values\n",
    "test_x  = test.values\n",
    "for i in result_train:\n",
    "    train_x = np.c_[train_x,i]\n",
    "for i in result_test:\n",
    "    test_x = np.c_[test_x,i]\n",
    "np.save('processed_data/train_np',train_x)\n",
    "np.save('processed_data/test_np',test_x)\n",
    "np.save('processed_data/target_np',train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%run learning.py"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
